{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE-CIC-IDS 2017 Ensembles - Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"voting4-softvoting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xetrov\\AppData\\Local\\Continuum\\anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Xetrov\\AppData\\Local\\Continuum\\anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Xetrov\\AppData\\Local\\Continuum\\anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\Xetrov\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Xetrov\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Xetrov\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Xetrov\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Xetrov\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Xetrov\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Xetrov\\AppData\\Local\\Continuum\\anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "import glob, pickle, time, keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_PATH = \"C:/Users/Xetrov/Desktop/SciFair20/Code/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled = pd.read_csv(NOTEBOOK_PATH + \"IDS2017/x_scaled_powertransform.csv\")\n",
    "\n",
    "y_df_enc = pd.read_csv(NOTEBOOK_PATH + \"IDS2017/y_all_binary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valtest, y_train, y_valtest = train_test_split(x_scaled, y_df_enc, test_size = 0.4, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, x_test, y_val, y_test = train_test_split(x_valtest, y_valtest, test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_valtest \n",
    "del y_valtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_res = pd.read_csv(NOTEBOOK_PATH + \"IDS2017/x_adasyn_binary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_res = pd.read_csv(NOTEBOOK_PATH + \"IDS2017/y_adasyn_binary.csv\")['IsAttack']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=300, random_state=42, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GradientBoostingClassifier(n_estimators=300, random_state=42, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = VotingClassifier([(\"Random Forest\", rf), (\"Gradient Boosted Trees\", gbt)], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   10.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 2 of 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   21.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 3 of 300\n",
      "building tree 4 of 300\n",
      "building tree 5 of 300\n",
      "building tree 6 of 300\n",
      "building tree 7 of 300\n",
      "building tree 8 of 300\n",
      "building tree 9 of 300\n",
      "building tree 10 of 300\n",
      "building tree 11 of 300\n",
      "building tree 12 of 300\n",
      "building tree 13 of 300\n",
      "building tree 14 of 300\n",
      "building tree 15 of 300\n",
      "building tree 16 of 300\n",
      "building tree 17 of 300\n",
      "building tree 18 of 300\n",
      "building tree 19 of 300\n",
      "building tree 20 of 300\n",
      "building tree 21 of 300\n",
      "building tree 22 of 300\n",
      "building tree 23 of 300\n",
      "building tree 24 of 300\n",
      "building tree 25 of 300\n",
      "building tree 26 of 300\n",
      "building tree 27 of 300\n",
      "building tree 28 of 300\n",
      "building tree 29 of 300\n",
      "building tree 30 of 300\n",
      "building tree 31 of 300\n",
      "building tree 32 of 300\n",
      "building tree 33 of 300\n",
      "building tree 34 of 300\n",
      "building tree 35 of 300\n",
      "building tree 36 of 300\n",
      "building tree 37 of 300\n",
      "building tree 38 of 300\n",
      "building tree 39 of 300\n",
      "building tree 40 of 300\n",
      "building tree 41 of 300\n",
      "building tree 42 of 300\n",
      "building tree 43 of 300\n",
      "building tree 44 of 300\n",
      "building tree 45 of 300\n",
      "building tree 46 of 300\n",
      "building tree 47 of 300\n",
      "building tree 48 of 300\n",
      "building tree 49 of 300\n",
      "building tree 50 of 300\n",
      "building tree 51 of 300\n",
      "building tree 52 of 300\n",
      "building tree 53 of 300\n",
      "building tree 54 of 300\n",
      "building tree 55 of 300\n",
      "building tree 56 of 300\n",
      "building tree 57 of 300\n",
      "building tree 58 of 300\n",
      "building tree 59 of 300\n",
      "building tree 60 of 300\n",
      "building tree 61 of 300\n",
      "building tree 62 of 300\n",
      "building tree 63 of 300\n",
      "building tree 64 of 300\n",
      "building tree 65 of 300\n",
      "building tree 66 of 300\n",
      "building tree 67 of 300\n",
      "building tree 68 of 300\n",
      "building tree 69 of 300\n",
      "building tree 70 of 300\n",
      "building tree 71 of 300\n",
      "building tree 72 of 300\n",
      "building tree 73 of 300\n",
      "building tree 74 of 300\n",
      "building tree 75 of 300\n",
      "building tree 76 of 300\n",
      "building tree 77 of 300\n",
      "building tree 78 of 300\n",
      "building tree 79 of 300\n",
      "building tree 80 of 300\n",
      "building tree 81 of 300\n",
      "building tree 82 of 300\n",
      "building tree 83 of 300\n",
      "building tree 84 of 300\n",
      "building tree 85 of 300\n",
      "building tree 86 of 300\n",
      "building tree 87 of 300\n",
      "building tree 88 of 300\n",
      "building tree 89 of 300\n",
      "building tree 90 of 300\n",
      "building tree 91 of 300\n",
      "building tree 92 of 300\n",
      "building tree 93 of 300\n",
      "building tree 94 of 300\n",
      "building tree 95 of 300\n",
      "building tree 96 of 300\n",
      "building tree 97 of 300\n",
      "building tree 98 of 300\n",
      "building tree 99 of 300\n",
      "building tree 100 of 300\n",
      "building tree 101 of 300\n",
      "building tree 102 of 300\n",
      "building tree 103 of 300\n",
      "building tree 104 of 300\n",
      "building tree 105 of 300\n",
      "building tree 106 of 300\n",
      "building tree 107 of 300\n",
      "building tree 108 of 300\n",
      "building tree 109 of 300\n",
      "building tree 110 of 300\n",
      "building tree 111 of 300\n",
      "building tree 112 of 300\n",
      "building tree 113 of 300\n",
      "building tree 114 of 300\n",
      "building tree 115 of 300\n",
      "building tree 116 of 300\n",
      "building tree 117 of 300\n",
      "building tree 118 of 300\n",
      "building tree 119 of 300\n",
      "building tree 120 of 300\n",
      "building tree 121 of 300\n",
      "building tree 122 of 300\n",
      "building tree 123 of 300\n",
      "building tree 124 of 300\n",
      "building tree 125 of 300\n",
      "building tree 126 of 300\n",
      "building tree 127 of 300\n",
      "building tree 128 of 300\n",
      "building tree 129 of 300\n",
      "building tree 130 of 300\n",
      "building tree 131 of 300\n",
      "building tree 132 of 300\n",
      "building tree 133 of 300\n",
      "building tree 134 of 300\n",
      "building tree 135 of 300\n",
      "building tree 136 of 300\n",
      "building tree 137 of 300\n",
      "building tree 138 of 300\n",
      "building tree 139 of 300\n",
      "building tree 140 of 300\n",
      "building tree 141 of 300\n",
      "building tree 142 of 300\n",
      "building tree 143 of 300\n",
      "building tree 144 of 300\n",
      "building tree 145 of 300\n",
      "building tree 146 of 300\n",
      "building tree 147 of 300\n",
      "building tree 148 of 300\n",
      "building tree 149 of 300\n",
      "building tree 150 of 300\n",
      "building tree 151 of 300\n",
      "building tree 152 of 300\n",
      "building tree 153 of 300\n",
      "building tree 154 of 300\n",
      "building tree 155 of 300\n",
      "building tree 156 of 300\n",
      "building tree 157 of 300\n",
      "building tree 158 of 300\n",
      "building tree 159 of 300\n",
      "building tree 160 of 300\n",
      "building tree 161 of 300\n",
      "building tree 162 of 300\n",
      "building tree 163 of 300\n",
      "building tree 164 of 300\n",
      "building tree 165 of 300\n",
      "building tree 166 of 300\n",
      "building tree 167 of 300\n",
      "building tree 168 of 300\n",
      "building tree 169 of 300\n",
      "building tree 170 of 300\n",
      "building tree 171 of 300\n",
      "building tree 172 of 300\n",
      "building tree 173 of 300\n",
      "building tree 174 of 300\n",
      "building tree 175 of 300\n",
      "building tree 176 of 300\n",
      "building tree 177 of 300\n",
      "building tree 178 of 300\n",
      "building tree 179 of 300\n",
      "building tree 180 of 300\n",
      "building tree 181 of 300\n",
      "building tree 182 of 300\n",
      "building tree 183 of 300\n",
      "building tree 184 of 300\n",
      "building tree 185 of 300\n",
      "building tree 186 of 300\n",
      "building tree 187 of 300\n",
      "building tree 188 of 300\n",
      "building tree 189 of 300\n",
      "building tree 190 of 300\n",
      "building tree 191 of 300\n",
      "building tree 192 of 300\n",
      "building tree 193 of 300\n",
      "building tree 194 of 300\n",
      "building tree 195 of 300\n",
      "building tree 196 of 300\n",
      "building tree 197 of 300\n",
      "building tree 198 of 300\n",
      "building tree 199 of 300\n",
      "building tree 200 of 300\n",
      "building tree 201 of 300\n",
      "building tree 202 of 300\n",
      "building tree 203 of 300\n",
      "building tree 204 of 300\n",
      "building tree 205 of 300\n",
      "building tree 206 of 300\n",
      "building tree 207 of 300\n",
      "building tree 208 of 300\n",
      "building tree 209 of 300\n",
      "building tree 210 of 300\n",
      "building tree 211 of 300\n",
      "building tree 212 of 300\n",
      "building tree 213 of 300\n",
      "building tree 214 of 300\n",
      "building tree 215 of 300\n",
      "building tree 216 of 300\n",
      "building tree 217 of 300\n",
      "building tree 218 of 300\n",
      "building tree 219 of 300\n",
      "building tree 220 of 300\n",
      "building tree 221 of 300\n",
      "building tree 222 of 300\n",
      "building tree 223 of 300\n",
      "building tree 224 of 300\n",
      "building tree 225 of 300\n",
      "building tree 226 of 300\n",
      "building tree 227 of 300\n",
      "building tree 228 of 300\n",
      "building tree 229 of 300\n",
      "building tree 230 of 300\n",
      "building tree 231 of 300\n",
      "building tree 232 of 300\n",
      "building tree 233 of 300\n",
      "building tree 234 of 300\n",
      "building tree 235 of 300\n",
      "building tree 236 of 300\n",
      "building tree 237 of 300\n",
      "building tree 238 of 300\n",
      "building tree 239 of 300\n",
      "building tree 240 of 300\n",
      "building tree 241 of 300\n",
      "building tree 242 of 300\n",
      "building tree 243 of 300\n",
      "building tree 244 of 300\n",
      "building tree 245 of 300\n",
      "building tree 246 of 300\n",
      "building tree 247 of 300\n",
      "building tree 248 of 300\n",
      "building tree 249 of 300\n",
      "building tree 250 of 300\n",
      "building tree 251 of 300\n",
      "building tree 252 of 300\n",
      "building tree 253 of 300\n",
      "building tree 254 of 300\n",
      "building tree 255 of 300\n",
      "building tree 256 of 300\n",
      "building tree 257 of 300\n",
      "building tree 258 of 300\n",
      "building tree 259 of 300\n",
      "building tree 260 of 300\n",
      "building tree 261 of 300\n",
      "building tree 262 of 300\n",
      "building tree 263 of 300\n",
      "building tree 264 of 300\n",
      "building tree 265 of 300\n",
      "building tree 266 of 300\n",
      "building tree 267 of 300\n",
      "building tree 268 of 300\n",
      "building tree 269 of 300\n",
      "building tree 270 of 300\n",
      "building tree 271 of 300\n",
      "building tree 272 of 300\n",
      "building tree 273 of 300\n",
      "building tree 274 of 300\n",
      "building tree 275 of 300\n",
      "building tree 276 of 300\n",
      "building tree 277 of 300\n",
      "building tree 278 of 300\n",
      "building tree 279 of 300\n",
      "building tree 280 of 300\n",
      "building tree 281 of 300\n",
      "building tree 282 of 300\n",
      "building tree 283 of 300\n",
      "building tree 284 of 300\n",
      "building tree 285 of 300\n",
      "building tree 286 of 300\n",
      "building tree 287 of 300\n",
      "building tree 288 of 300\n",
      "building tree 289 of 300\n",
      "building tree 290 of 300\n",
      "building tree 291 of 300\n",
      "building tree 292 of 300\n",
      "building tree 293 of 300\n",
      "building tree 294 of 300\n",
      "building tree 295 of 300\n",
      "building tree 296 of 300\n",
      "building tree 297 of 300\n",
      "building tree 298 of 300\n",
      "building tree 299 of 300\n",
      "building tree 300 of 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed: 45.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2491           60.41m\n",
      "         2           1.1369           60.40m\n",
      "         3           1.0437           60.18m\n",
      "         4           0.9535           59.91m\n",
      "         5           0.8887           59.74m\n",
      "         6           0.8288           59.58m\n",
      "         7           0.7753           59.25m\n",
      "         8           0.7298           58.93m\n",
      "         9           0.6904           58.64m\n",
      "        10           0.6523           58.53m\n",
      "        11           0.6119           58.39m\n",
      "        12           0.5751           58.28m\n",
      "        13           0.5512           58.22m\n",
      "        14           0.5224           58.03m\n",
      "        15           0.4934           57.79m\n",
      "        16           0.4779           57.68m\n",
      "        17           0.4546           57.42m\n",
      "        18           0.4350           57.24m\n",
      "        19           0.4155           57.05m\n",
      "        20           0.3986           56.82m\n",
      "        21           0.3858           56.68m\n",
      "        22           0.3717           56.45m\n",
      "        23           0.3581           56.28m\n",
      "        24           0.3468           56.05m\n",
      "        25           0.3362           55.87m\n",
      "        26           0.3265           55.64m\n",
      "        27           0.3177           55.46m\n",
      "        28           0.3094           55.24m\n",
      "        29           0.3012           55.01m\n",
      "        30           0.2940           54.83m\n",
      "        31           0.2868           54.65m\n",
      "        32           0.2799           54.43m\n",
      "        33           0.2745           54.18m\n",
      "        34           0.2675           53.95m\n",
      "        35           0.2633           53.70m\n",
      "        36           0.2584           53.60m\n",
      "        37           0.2545           53.37m\n",
      "        38           0.2487           53.11m\n",
      "        39           0.2446           52.92m\n",
      "        40           0.2402           52.64m\n",
      "        41           0.2360           52.43m\n",
      "        42           0.2318           52.22m\n",
      "        43           0.2274           52.01m\n",
      "        44           0.2254           51.77m\n",
      "        45           0.2209           51.65m\n",
      "        46           0.2174           51.47m\n",
      "        47           0.2152           51.20m\n",
      "        48           0.2124           50.98m\n",
      "        49           0.2109           50.72m\n",
      "        50           0.2079           50.51m\n",
      "        51           0.2052           50.28m\n",
      "        52           0.2030           50.04m\n",
      "        53           0.2005           49.86m\n",
      "        54           0.1988           49.62m\n",
      "        55           0.1964           49.36m\n",
      "        56           0.1941           49.21m\n",
      "        57           0.1916           49.00m\n",
      "        58           0.1903           48.77m\n",
      "        59           0.1866           48.56m\n",
      "        60           0.1845           48.34m\n",
      "        61           0.1803           48.15m\n",
      "        62           0.1788           47.99m\n",
      "        63           0.1779           47.73m\n",
      "        64           0.1766           47.52m\n",
      "        65           0.1749           47.36m\n",
      "        66           0.1739           47.11m\n",
      "        67           0.1728           46.88m\n",
      "        68           0.1706           46.67m\n",
      "        69           0.1674           46.47m\n",
      "        70           0.1655           46.24m\n",
      "        71           0.1636           46.09m\n",
      "        72           0.1604           45.89m\n",
      "        73           0.1595           45.66m\n",
      "        74           0.1567           45.46m\n",
      "        75           0.1557           45.23m\n",
      "        76           0.1541           45.02m\n",
      "        77           0.1513           44.82m\n",
      "        78           0.1503           44.66m\n",
      "        79           0.1476           44.46m\n",
      "        80           0.1467           44.23m\n",
      "        81           0.1454           44.02m\n",
      "        82           0.1446           43.85m\n",
      "        83           0.1420           43.66m\n",
      "        84           0.1416           43.42m\n",
      "        85           0.1408           43.19m\n",
      "        86           0.1394           42.99m\n",
      "        87           0.1371           42.80m\n",
      "        88           0.1366           42.59m\n",
      "        89           0.1354           42.36m\n",
      "        90           0.1347           42.14m\n",
      "        91           0.1337           41.93m\n",
      "        92           0.1325           41.71m\n",
      "        93           0.1319           41.49m\n",
      "        94           0.1314           41.26m\n",
      "        95           0.1305           41.09m\n",
      "        96           0.1297           40.88m\n",
      "        97           0.1287           40.67m\n",
      "        98           0.1269           40.47m\n",
      "        99           0.1258           40.27m\n",
      "       100           0.1238           40.08m\n",
      "       101           0.1235           39.88m\n",
      "       102           0.1229           39.66m\n",
      "       103           0.1220           39.45m\n",
      "       104           0.1218           39.22m\n",
      "       105           0.1198           39.06m\n",
      "       106           0.1190           38.85m\n",
      "       107           0.1185           38.63m\n",
      "       108           0.1171           38.44m\n",
      "       109           0.1151           38.24m\n",
      "       110           0.1144           38.06m\n",
      "       111           0.1131           37.86m\n",
      "       112           0.1129           37.64m\n",
      "       113           0.1115           37.44m\n",
      "       114           0.1108           37.23m\n",
      "       115           0.1105           37.01m\n",
      "       116           0.1103           36.79m\n",
      "       117           0.1092           36.59m\n",
      "       118           0.1087           36.38m\n",
      "       119           0.1070           36.17m\n",
      "       120           0.1054           35.98m\n",
      "       121           0.1048           35.80m\n",
      "       122           0.1043           35.58m\n",
      "       123           0.1038           35.38m\n",
      "       124           0.1032           35.17m\n",
      "       125           0.1020           34.99m\n",
      "       126           0.1007           34.79m\n",
      "       127           0.1004           34.58m\n",
      "       128           0.0988           34.38m\n",
      "       129           0.0982           34.20m\n",
      "       130           0.0979           33.99m\n",
      "       131           0.0974           33.78m\n",
      "       132           0.0968           33.57m\n",
      "       133           0.0966           33.36m\n",
      "       134           0.0959           33.15m\n",
      "       135           0.0952           32.95m\n",
      "       136           0.0949           32.74m\n",
      "       137           0.0946           32.54m\n",
      "       138           0.0942           32.33m\n",
      "       139           0.0936           32.13m\n",
      "       140           0.0929           31.94m\n",
      "       141           0.0917           31.76m\n",
      "       142           0.0914           31.55m\n",
      "       143           0.0912           31.35m\n",
      "       144           0.0911           31.14m\n",
      "       145           0.0906           30.94m\n",
      "       146           0.0903           30.74m\n",
      "       147           0.0901           30.54m\n",
      "       148           0.0898           30.34m\n",
      "       149           0.0892           30.13m\n",
      "       150           0.0883           29.94m\n",
      "       151           0.0879           29.74m\n",
      "       152           0.0874           29.54m\n",
      "       153           0.0870           29.35m\n",
      "       154           0.0861           29.15m\n",
      "       155           0.0860           28.95m\n",
      "       156           0.0853           28.75m\n",
      "       157           0.0852           28.54m\n",
      "       158           0.0848           28.35m\n",
      "       159           0.0844           28.14m\n",
      "       160           0.0842           27.93m\n",
      "       161           0.0840           27.72m\n",
      "       162           0.0836           27.52m\n",
      "       163           0.0829           27.32m\n",
      "       164           0.0817           27.13m\n",
      "       165           0.0810           26.93m\n",
      "       166           0.0802           26.73m\n",
      "       167           0.0798           26.53m\n",
      "       168           0.0790           26.33m\n",
      "       169           0.0787           26.13m\n",
      "       170           0.0785           25.92m\n",
      "       171           0.0783           25.71m\n",
      "       172           0.0781           25.50m\n",
      "       173           0.0778           25.29m\n",
      "       174           0.0772           25.09m\n",
      "       175           0.0769           24.90m\n",
      "       176           0.0766           24.70m\n",
      "       177           0.0757           24.50m\n",
      "       178           0.0756           24.30m\n",
      "       179           0.0749           24.12m\n",
      "       180           0.0747           23.92m\n",
      "       181           0.0745           23.71m\n",
      "       182           0.0743           23.51m\n",
      "       183           0.0740           23.31m\n",
      "       184           0.0737           23.11m\n",
      "       185           0.0735           22.91m\n",
      "       186           0.0732           22.70m\n",
      "       187           0.0730           22.50m\n",
      "       188           0.0725           22.30m\n",
      "       189           0.0722           22.10m\n",
      "       190           0.0721           21.89m\n",
      "       191           0.0717           21.70m\n",
      "       192           0.0711           21.51m\n",
      "       193           0.0710           21.30m\n",
      "       194           0.0707           21.10m\n",
      "       195           0.0701           20.90m\n",
      "       196           0.0697           20.71m\n",
      "       197           0.0694           20.51m\n",
      "       198           0.0692           20.31m\n",
      "       199           0.0688           20.11m\n",
      "       200           0.0684           19.91m\n",
      "       201           0.0683           19.70m\n",
      "       202           0.0682           19.50m\n",
      "       203           0.0677           19.30m\n",
      "       204           0.0674           19.10m\n",
      "       205           0.0672           18.91m\n",
      "       206           0.0672           18.71m\n",
      "       207           0.0670           18.51m\n",
      "       208           0.0668           18.30m\n",
      "       209           0.0665           18.11m\n",
      "       210           0.0662           17.91m\n",
      "       211           0.0661           17.71m\n",
      "       212           0.0653           17.51m\n",
      "       213           0.0650           17.31m\n",
      "       214           0.0647           17.11m\n",
      "       215           0.0645           16.91m\n",
      "       216           0.0642           16.71m\n",
      "       217           0.0640           16.50m\n",
      "       218           0.0639           16.30m\n",
      "       219           0.0637           16.11m\n",
      "       220           0.0634           15.91m\n",
      "       221           0.0632           15.72m\n",
      "       222           0.0629           15.51m\n",
      "       223           0.0627           15.32m\n",
      "       224           0.0621           15.12m\n",
      "       225           0.0615           14.93m\n",
      "       226           0.0613           14.73m\n",
      "       227           0.0612           14.52m\n",
      "       228           0.0610           14.32m\n",
      "       229           0.0607           14.13m\n",
      "       230           0.0604           13.93m\n",
      "       231           0.0603           13.73m\n",
      "       232           0.0601           13.53m\n",
      "       233           0.0600           13.33m\n",
      "       234           0.0599           13.12m\n",
      "       235           0.0596           12.92m\n",
      "       236           0.0592           12.73m\n",
      "       237           0.0591           12.53m\n",
      "       238           0.0587           12.33m\n",
      "       239           0.0584           12.13m\n",
      "       240           0.0581           11.93m\n",
      "       241           0.0576           11.73m\n",
      "       242           0.0575           11.53m\n",
      "       243           0.0574           11.33m\n",
      "       244           0.0568           11.14m\n",
      "       245           0.0566           10.94m\n",
      "       246           0.0565           10.74m\n",
      "       247           0.0564           10.54m\n",
      "       248           0.0561           10.34m\n",
      "       249           0.0560           10.13m\n",
      "       250           0.0559            9.93m\n",
      "       251           0.0557            9.73m\n",
      "       252           0.0553            9.54m\n",
      "       253           0.0548            9.34m\n",
      "       254           0.0546            9.14m\n",
      "       255           0.0545            8.94m\n",
      "       256           0.0540            8.75m\n",
      "       257           0.0539            8.54m\n",
      "       258           0.0537            8.34m\n",
      "       259           0.0533            8.14m\n",
      "       260           0.0532            7.95m\n",
      "       261           0.0531            7.75m\n",
      "       262           0.0528            7.55m\n",
      "       263           0.0528            7.35m\n",
      "       264           0.0525            7.15m\n",
      "       265           0.0524            6.95m\n",
      "       266           0.0523            6.75m\n",
      "       267           0.0522            6.55m\n",
      "       268           0.0515            6.35m\n",
      "       269           0.0513            6.15m\n",
      "       270           0.0512            5.95m\n",
      "       271           0.0510            5.75m\n",
      "       272           0.0509            5.56m\n",
      "       273           0.0507            5.36m\n",
      "       274           0.0506            5.16m\n",
      "       275           0.0506            4.96m\n",
      "       276           0.0505            4.76m\n",
      "       277           0.0504            4.56m\n",
      "       278           0.0501            4.36m\n",
      "       279           0.0498            4.16m\n",
      "       280           0.0497            3.96m\n",
      "       281           0.0493            3.77m\n",
      "       282           0.0493            3.57m\n",
      "       283           0.0492            3.37m\n",
      "       284           0.0491            3.17m\n",
      "       285           0.0490            2.97m\n",
      "       286           0.0490            2.77m\n",
      "       287           0.0487            2.58m\n",
      "       288           0.0484            2.38m\n",
      "       289           0.0480            2.18m\n",
      "       290           0.0480            1.98m\n",
      "       291           0.0478            1.78m\n",
      "       292           0.0476            1.59m\n",
      "       293           0.0474            1.39m\n",
      "       294           0.0472            1.19m\n",
      "       295           0.0469           59.47s\n",
      "       296           0.0468           47.59s\n",
      "       297           0.0467           35.69s\n",
      "       298           0.0466           23.78s\n",
      "       299           0.0463           11.89s\n",
      "       300           0.0463            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('Random Forest',\n",
       "                              RandomForestClassifier(bootstrap=True,\n",
       "                                                     class_weight=None,\n",
       "                                                     criterion='gini',\n",
       "                                                     max_depth=None,\n",
       "                                                     max_features='auto',\n",
       "                                                     max_leaf_nodes=None,\n",
       "                                                     min_impurity_decrease=0.0,\n",
       "                                                     min_impurity_split=None,\n",
       "                                                     min_samples_leaf=1,\n",
       "                                                     min_samples_split=2,\n",
       "                                                     min_weight_fraction_leaf=0.0,\n",
       "                                                     n_estimators=300,\n",
       "                                                     n_jobs=None,\n",
       "                                                     oob_score=False,\n",
       "                                                     random_state=4...\n",
       "                                                         max_features=None,\n",
       "                                                         max_leaf_nodes=None,\n",
       "                                                         min_impurity_decrease=0.0,\n",
       "                                                         min_impurity_split=None,\n",
       "                                                         min_samples_leaf=1,\n",
       "                                                         min_samples_split=2,\n",
       "                                                         min_weight_fraction_leaf=0.0,\n",
       "                                                         n_estimators=300,\n",
       "                                                         n_iter_no_change=None,\n",
       "                                                         presort='auto',\n",
       "                                                         random_state=42,\n",
       "                                                         subsample=1.0,\n",
       "                                                         tol=0.0001,\n",
       "                                                         validation_fraction=0.1,\n",
       "                                                         verbose=2,\n",
       "                                                         warm_start=False))],\n",
       "                 flatten_transform=True, n_jobs=None, voting='soft',\n",
       "                 weights=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble.fit(x_train_res, y_train_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = model_id  + \" [\" + time.strftime(\"%Y%m%d %H%M\") + \"]\"\n",
    "pickle.dump(ensemble, open(NOTEBOOK_PATH + \"Models/\" + model_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 1, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ensemble.predict(x_test[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:   19.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9930525208023046\n",
      "Recall: 0.9999012203773382\n",
      "F1: 0.9964651029594694\n"
     ]
    }
   ],
   "source": [
    "pred = ensemble.predict(x_test)\n",
    "y_test_npy = y_test.to_numpy().ravel()\n",
    "\n",
    "precision = precision_score(y_test_npy, pred)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "recall = recall_score(y_test_npy, pred)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "f1 = f1_score(y_test_npy, pred)\n",
    "print(\"F1:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Actual</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Benign</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pred</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Attack</th>\n",
       "      <td>111348</td>\n",
       "      <td>779</td>\n",
       "      <td>112127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Benign</th>\n",
       "      <td>11</td>\n",
       "      <td>454011</td>\n",
       "      <td>454022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>111359</td>\n",
       "      <td>454790</td>\n",
       "      <td>566149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Actual  Attack  Benign     All\n",
       "Pred                          \n",
       "Attack  111348     779  112127\n",
       "Benign      11  454011  454022\n",
       "All     111359  454790  566149"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_series = pd.Series(pred, name=\"Pred\").replace({0: 'Benign', 1: 'Attack'})\n",
    "y_series = pd.Series(y_test.to_numpy().ravel(), name=\"Actual\").replace({0: 'Benign', 1: 'Attack'})\n",
    "\n",
    "matrix = pd.crosstab(pred_series, y_series, margins=True)\n",
    "matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
